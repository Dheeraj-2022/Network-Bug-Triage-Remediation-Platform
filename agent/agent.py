#!/usr/bin/env python3
"""
Network Bug Triage - Agent (production-ready)

Responsibilities:
- Collect telemetry periodically:
    - kernel logs (dmesg tail)
    - interface statistics (psutil)
    - RDMA counters (rdma CLI) [optional]
    - small packet-sampling (bcc/eBPF if present) [optional]
- Build JSON event and publish to Kafka topic (kafka-python)
- Provide local file-backed buffering for reliability when Kafka is unavailable
- Support dry-run mode for quick testing (print events)
- Designed to be run under systemd (service file provided)

Usage (dev):
    python3 agent/agent.py --dry-run --interval 5
    python3 agent/agent.py --once --config agent/config.yaml

Service (systemd):
    /etc/systemd/system/network-triage-agent.service -> starts python wrapper above

Author: Generated by assistant (adapt to your environment)
"""

import argparse
import json
import logging
import os
import socket
import subprocess
import sys
import threading
import time
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

# Optional imports (use gracefully)
try:
    import yaml
except Exception:
    yaml = None

try:
    import psutil
except Exception:
    psutil = None

try:
    from kafka import KafkaProducer
except ImportError:
    KafkaProducer = None
except Exception as e:
    logger.warning("Failed to import KafkaProducer: %s", e)
    KafkaProducer = None

# Constants
DEFAULT_CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
DEFAULT_BUFFER_DIR = "/var/lib/network-bug-triage"
DEFAULT_BUFFER_FILE = os.path.join(DEFAULT_BUFFER_DIR, "agent-queue.jsonl")

# Logging setup (basic; reconfigured later after config load)
logger = logging.getLogger("nbt.agent")
logger.setLevel(logging.INFO)
_stream = logging.StreamHandler(sys.stdout)
_stream.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
logger.addHandler(_stream)


def iso_now():
    return datetime.now(timezone.utc).isoformat()


class Config:
    def __init__(self, path: Optional[str] = None):
        self.path = path or DEFAULT_CONFIG_PATH
        self._c = {}
        self.load_defaults()

    def load_defaults(self):
        self._c = {
            "kafka": {
                "bootstrap_servers": ["localhost:9092"],
                "topic": "telemetry.events",
                "acks": "all",
                "retries": 5,
                "linger_ms": 100
            },
            "collect": {
                "interval_sec": 5,
                "dmesg_tail_lines": 20,
                "sample_packets": 2,
                "enable_ebpf": False,
                "enable_rdma": True
            },
            "agent": {
                "host_id": None,
                "dry_run": True,
                "buffering": {
                    "enabled": True,
                    "path": DEFAULT_BUFFER_FILE,
                    "max_pending": 10000
                }
            },
            "logging": {
                "level": "INFO",
                "file": None
            }
        }

    def load(self, path: Optional[str] = None):
        p = path or self.path
        if not os.path.exists(p):
            logger.warning("Config file not found at %s; using defaults", p)
            return
        if yaml is None:
            raise RuntimeError("pyyaml required to load YAML config")
        with open(p, "r") as fh:
            usercfg = yaml.safe_load(fh) or {}
            # deep update
            self._deep_update(self._c, usercfg)

    def _deep_update(self, orig, new):
        for k, v in (new or {}).items():
            if isinstance(v, dict) and k in orig and isinstance(orig[k], dict):
                self._deep_update(orig[k], v)
            else:
                orig[k] = v

    def get(self, *keys, default=None):
        cur = self._c
        for k in keys:
            if isinstance(cur, dict) and k in cur:
                cur = cur[k]
            else:
                return default
        return cur

    def as_dict(self):
        return self._c


class FileBufferQueue:
    """
    Simple append-only JSONL file as persistent queue.
    Not fully robust (no compaction), but sufficient for agent offline buffering.
    Append format: one JSON per line.
    """

    def __init__(self, path: str, max_pending: int = 10000):
        self.path = path
        self.max_pending = int(max_pending)
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        # ensure file exists
        open(self.path, "a").close()
        self.lock = threading.Lock()

    def push(self, obj: Dict[str, Any]):
        line = json.dumps(obj, separators=(",", ":"))
        with self.lock:
            with open(self.path, "a") as fh:
                fh.write(line + "\n")
        logger.debug("Buffered event to %s", self.path)
        self._enforce_limit()

    def pop_all(self) -> List[Dict[str, Any]]:
        """
        Read and clear file atomically (simple approach):
        - read all lines
        - truncate file
        - return list of parsed json objects
        """
        with self.lock:
            with open(self.path, "r") as fh:
                lines = fh.readlines()
            # truncate
            open(self.path, "w").close()
        events = []
        for line in lines:
            try:
                events.append(json.loads(line))
            except Exception:
                logger.exception("Failed to parse buffered line")
        logger.debug("Popped %d buffered events", len(events))
        return events

    def pending_count(self) -> int:
        with self.lock:
            with open(self.path, "r") as fh:
                return sum(1 for _ in fh)

    def _enforce_limit(self):
        # if file grows beyond max_pending, drop older lines (naive)
        try:
            with self.lock:
                with open(self.path, "r") as fh:
                    lines = fh.readlines()
                if len(lines) <= self.max_pending:
                    return
                # keep last max_pending lines
                tail = lines[-self.max_pending :]
                with open(self.path, "w") as fh:
                    fh.writelines(tail)
                logger.warning("Buffer file exceeded max_pending; truncated older entries")
        except Exception:
            logger.exception("Failed enforcing buffer limit")


class TelemetryProducer:
    def __init__(self, bootstrap_servers: List[str], topic: str, dry_run: bool = True, buffering_cfg: Dict = None):
        self.bootstrap_servers = bootstrap_servers
        self.topic = topic
        self.dry_run = dry_run
        self.buffering_cfg = buffering_cfg or {"enabled": False}
        self.buffer = None
        self.producer = None
        if self.buffering_cfg.get("enabled"):
            path = self.buffering_cfg.get("path", DEFAULT_BUFFER_FILE)
            max_pending = self.buffering_cfg.get("max_pending", 10000)
            self.buffer = FileBufferQueue(path, max_pending=max_pending)
        if not self.dry_run:
            if KafkaProducer is None:
                raise RuntimeError("kafka-python package required for real Kafka publishing")
            # kafka-python producer basic config
            self.producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode("utf-8"),
                linger_ms=int(self.buffering_cfg.get("linger_ms", 100)) if isinstance(self.buffering_cfg.get("linger_ms"), int) else 100,
                retries=int(self.buffering_cfg.get("retries", 5)) if self.buffering_cfg.get("retries") is not None else 5
            )

    def send(self, event: Dict[str, Any]) -> bool:
        if self.dry_run:
            logger.info("DRY-RUN event -> %s", json.dumps(event))
            return True
        try:
            fut = self.producer.send(self.topic, event)
            fut.get(timeout=10)
            logger.debug("Published event %s to %s", event.get("event_id"), self.topic)
            return True
        except Exception:
            logger.exception("Failed to publish to Kafka; buffering locally if enabled")
            if self.buffer:
                try:
                    self.buffer.push(event)
                    return True
                except Exception:
                    logger.exception("Buffer push failed")
                    return False
            return False

    def flush_buffer(self):
        if not self.buffer or self.dry_run:
            return
        pending = self.buffer.pop_all()
        if not pending:
            return
        logger.info("Flushing %d buffered events to Kafka", len(pending))
        for ev in pending:
            try:
                fut = self.producer.send(self.topic, ev)
                fut.get(timeout=10)
            except Exception:
                logger.exception("Failed to send buffered event; re-buffering remaining")
                # re-buffer rest (append)
                for r in pending[pending.index(ev) :]:
                    self.buffer.push(r)
                return


# --- Telemetry collectors ---

def collect_iface_stats() -> Dict[str, Dict[str, int]]:
    """
    Collect interface statistics using psutil if available; otherwise fall back to /sys/class/net parsing.
    Returns dict: iface -> counters
    """
    stats = {}
    if psutil:
        try:
            nic_stats = psutil.net_io_counters(pernic=True)
            for nic, c in nic_stats.items():
                stats[nic] = {
                    "rx_bytes": int(getattr(c, "bytes_recv", 0)),
                    "tx_bytes": int(getattr(c, "bytes_sent", 0)),
                    "errin": int(getattr(c, "errin", 0)) if hasattr(c, "errin") else 0,
                    "errout": int(getattr(c, "errout", 0)) if hasattr(c, "errout") else 0,
                    "mtu": _read_iface_mtu(nic)
                }
            return stats
        except Exception:
            logger.exception("psutil interface read failed; falling back")
    # fallback: read from /sys/class/net
    try:
        net_dir = "/sys/class/net"
        for nic in os.listdir(net_dir):
            try:
                with open(os.path.join(net_dir, nic, "statistics", "rx_bytes"), "r") as fh:
                    rx = int(fh.read().strip())
                with open(os.path.join(net_dir, nic, "statistics", "tx_bytes"), "r") as fh:
                    tx = int(fh.read().strip())
                # errin/errout may not exist; try:
                errin = _safe_read_int(os.path.join(net_dir, nic, "statistics", "rx_errors"))
                errout = _safe_read_int(os.path.join(net_dir, nic, "statistics", "tx_errors"))
                mtu = _read_iface_mtu(nic)
                stats[nic] = {"rx_bytes": rx, "tx_bytes": tx, "errin": errin, "errout": errout, "mtu": mtu}
            except Exception:
                continue
    except Exception:
        logger.exception("Fallback net stats collection failed")
    return stats


def _safe_read_int(path: str) -> int:
    try:
        with open(path, "r") as fh:
            return int(fh.read().strip())
    except Exception:
        return 0


def _read_iface_mtu(iface: str) -> int:
    try:
        with open(f"/sys/class/net/{iface}/mtu", "r") as fh:
            return int(fh.read().strip())
    except Exception:
        # fallback ip command
        try:
            out = subprocess.check_output(["ip", "-o", "link", "show", iface], text=True)
            # output: '2: eth0: <...> mtu 1500 ...'
            parts = out.split()
            if "mtu" in parts:
                return int(parts[parts.index("mtu") + 1])
        except Exception:
            pass
    return 1500


def collect_rdma_stats() -> Dict[str, int]:
    """
    Try to gather RDMA counters using `rdma` CLI (from rdma-core) if available.
    Fallback returns zeros.
    """
    res = {"qp_errors": 0, "rq_errors": 0, "srq_errors": 0}
    try:
        # Attempt simple rdma dev show parsing (best effort)
        out = subprocess.check_output(["rdma", "dev", "show"], stderr=subprocess.DEVNULL, text=True)
        # crude heuristic: count lines with 'error' or 'qp' mentions
        lower = out.lower()
        if "error" in lower:
            res["qp_errors"] = lower.count("error")
        # There is no universal standard format; this is a placeholder
        return res
    except FileNotFoundError:
        logger.debug("rdma CLI not present; skipping RDMA counters")
    except Exception:
        logger.exception("Failed to collect RDMA counters")
    return res


def tail_dmesg(max_lines: int = 20) -> str:
    """
    Return last N lines of dmesg with ISO timestamps where available.
    Uses 'dmesg --time-format iso' if supported.
    """
    try:
        out = subprocess.check_output(["dmesg", "--time-format", "iso"], stderr=subprocess.DEVNULL, text=True)
        lines = out.strip().splitlines()
        return "\n".join(lines[-max_lines:])
    except Exception:
        try:
            out = subprocess.check_output(["dmesg"], stderr=subprocess.DEVNULL, text=True)
            lines = out.strip().splitlines()
            return "\n".join(lines[-max_lines:])
        except Exception:
            return ""


def sample_packets(max_samples: int = 2, enable_ebpf: bool = False) -> List[Dict[str, Any]]:
    """
    Placeholder for packet sampling.
    If bcc is installed and enable_ebpf True, you could attach an eBPF program to sample packets.
    For portability we return small synthetic samples here.
    """
    if enable_ebpf:
        # Implement eBPF sample in future iterations (requires bcc and elevated privileges)
        logger.debug("eBPF requested but not implemented in this agent release; returning placeholder samples")
    return [
        {"src": "10.0.0.1", "dst": "10.0.0.2", "len": 128, "proto": "TCP"},
        {"src": "10.0.0.2", "dst": "10.0.0.1", "len": 64, "proto": "ICMP"},
    ][:max_samples]


def build_event(host_id_override: Optional[str], cfg: Config) -> Dict[str, Any]:
    host_id = host_id_override or cfg.get("agent", "host_id") or socket.gethostname()
    event = {
        "event_id": str(uuid.uuid4()),
        "host": host_id,
        "ts": iso_now(),
        "ifaces": collect_iface_stats(),
        "rdma": collect_rdma_stats() if cfg.get("collect", "enable_rdma") else {"qp_errors": 0, "rq_errors": 0, "srq_errors": 0},
        "dmesg_tail": tail_dmesg(cfg.get("collect", "dmesg_tail_lines") or 20),
        "sample_packets": sample_packets(cfg.get("collect", "sample_packets") or 2, enable_ebpf=cfg.get("collect", "enable_ebpf")),
    }
    return event


def configure_logging(cfg: Config):
    level = cfg.get("logging", "level", default="INFO") if hasattr(cfg, "get") else "INFO"
    logfile = cfg.get("logging", "file", default=None)
    numeric = getattr(logging, level.upper(), logging.INFO)
    logger.setLevel(numeric)
    # remove extra handlers and set stream handler if not present
    # keep the initial stream handler; add file handler if requested
    if logfile:
        try:
            import os
            logdir = os.path.dirname(logfile)
            if logdir and not os.path.exists(logdir):
                os.makedirs(logdir, exist_ok=True)
            fh = logging.FileHandler(logfile)
            fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
            logger.addHandler(fh)
        except Exception:
            logger.exception("Failed to open log file %s; continuing with stdout", logfile)


def main_loop(cfg: Config, args):
    producer_cfg = cfg.get("kafka") or {}
    buffering_cfg = cfg.get("agent", "buffering") or {}
    bootstrap = producer_cfg.get("bootstrap_servers", ["localhost:9092"])
    topic = producer_cfg.get("topic", "telemetry.events")
    dry = args.dry_run or cfg.get("agent", "dry_run", default=True)
    host_id = args.host_id or cfg.get("agent", "host_id")
    interval = int(args.interval if args.interval is not None else cfg.get("collect", "interval_sec", default=5))
    # merge linger/retries options into buffering cfg for producer creation
    buffering_cfg["linger_ms"] = producer_cfg.get("linger_ms", 100)
    buffering_cfg["retries"] = producer_cfg.get("retries", 5)

    producer = TelemetryProducer(bootstrap, topic, dry_run=dry, buffering_cfg=buffering_cfg)
    logger.info("Agent starting (dry_run=%s) host=%s interval=%ds", dry, host_id or socket.gethostname(), interval)

    stop_event = threading.Event()

    def periodic_flush():
        """
        Background thread: try to flush buffered events every 10s
        """
        while not stop_event.wait(10):
            try:
                if not dry:
                    producer.flush_buffer()
            except Exception:
                logger.exception("Buffer flush failed")

    flusher = threading.Thread(target=periodic_flush, daemon=True)
    flusher.start()

    try:
        if args.once:
            ev = build_event(host_id, cfg)
            ok = producer.send(ev)
            if not ok:
                logger.warning("Failed to send event (once mode)")
            return 0
        while True:
            ev = build_event(host_id, cfg)
            ok = producer.send(ev)
            if not ok:
                logger.warning("Event send failed; event buffered if configured")
            # sleep in smaller increments to be responsive to signals if desired
            for _ in range(int(interval)):
                time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Agent received KeyboardInterrupt; shutting down")
    except Exception:
        logger.exception("Unhandled exception in agent main loop")
        return 1
    finally:
        stop_event.set()
        # final flush attempt
        try:
            producer.flush_buffer()
        except Exception:
            pass
    return 0


def parse_args():
    p = argparse.ArgumentParser(description="Network Bug Triage - Agent")
    p.add_argument("--config", "-c", help="Path to YAML config", default=None)
    p.add_argument("--dry-run", action="store_true", help="Print events instead of sending to Kafka")
    p.add_argument("--interval", "-i", type=int, help="Override collection interval (seconds)")
    p.add_argument("--host-id", help="Override host id string")
    p.add_argument("--once", action="store_true", help="Collect once and exit")
    return p.parse_args()


def main():
    args = parse_args()
    cfg = Config(args.config or DEFAULT_CONFIG_PATH)
    try:
        cfg.load(args.config or DEFAULT_CONFIG_PATH)
    except Exception as e:
        logger.warning("Failed to load config: %s", e)
    configure_logging(cfg)
    code = main_loop(cfg, args)
    sys.exit(code)


if __name__ == "__main__":
    main()
